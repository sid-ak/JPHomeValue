{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KRlH60U7gK2I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647023408175,"user_tz":300,"elapsed":17417,"user":{"displayName":"Kejvi Cupa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14850223776994818767"}},"outputId":"14006e76-a364-4f53-b4c1-b7140ae7271c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import drive, files # google colab specific\n","import requests\n","import pandas as pd\n","import os\n","import warnings\n","import sys\n","import matplotlib.pyplot as plt\n","import json\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","import math\n","from sklearn.metrics import mean_squared_error"],"metadata":{"id":"UQOo3N9QgiiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read in the csv file that contains the time series data of the Shiller Index\n","# Create the dataframe\n","\n","df = pd.read_csv('')\n","df.head()"],"metadata":{"id":"NP79JZh5gikb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reset the index based on the prices\n","df1 = df.reset_index()['index value']\n","df1"],"metadata":{"id":"nymcV0xmgim7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the data for reference. \n","plt.plot(df1)"],"metadata":{"id":"7mAcQZqegipe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalize the data with the range 0 - 1\n","scaler = MinMaxScaler(feature_range=(0,1))\n","\n","# Apply the scaler to the dataframe\n","df1 = scaler.fit_transform(np.array(df1).reshape(-1,1))\n","\n","# Print df1 to check the change\n","print(df1)"],"metadata":{"id":"HQrFNXdlgiru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into train, validation and test set. Ratio: 7/2/1\n","training_size = int(len(df1)*0.7)\n","validation_size = int(len(df1)*0.9) \n","test_size = int(len(df1))\n","\n","train_set = df1[0:training_size,:]\n","validation_set = df1[training_size:validation_size,:]\n","test_set = df1[validation_size:test_size,:1]\n","\n"],"metadata":{"id":"7-RFY8rPgit_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check sizes and set sizes\n","training_size, validation_size, test_size\n","train_set.shape\n","validation_set.shape\n","test_set.shape"],"metadata":{"id":"NFMvq31VgiwY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The current array of values needs to be converted into a dataset matrix\n","def create_dmatrix(dataset, time_step=1):\n","  data_X, data_Y = [], []\n","  for i in range(len(dataset)-time_step-1):\n","    k = dataset[i:(i + time_step), 0]\n","    data_X.append(k)\n","    data_Y.append(dataset[i + time_step, 0])\n","  return np.array(data_X), np.array(data_Y)"],"metadata":{"id":"FTaWKxTkgiy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using the function already created for the conversion, convert training, validation and test sets. \n","time_step = 20\n","X_train, Y_train = create_dmatrix(train_set, time_step)\n","X_val, Y_val = create_dmatrix(validation_set, time_step)\n","X_test, Y_test = create_dmatrix(test_set, time_step)\n","\n","print('X Train set shape:', X_train.shape)\n","print('y Train set shape:', Y_train.shape)\n","\n","print('X Validation set shape:', X_val.shape)\n","print('y Validation set shape:', Y_val.shape)\n","\n","print('X Test set shape:', X_test.shape)\n","print('y Test set shape:', Y_test.shape)\n"],"metadata":{"id":"GNnJWZcwgi8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In order to feed the dataset to the neural network the data must be in 3D\n","\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"],"metadata":{"id":"NTkZBoTbmRZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","batch_size = 64\n","epochs = 500\n","learning_rate = 0.001"],"metadata":{"id":"OVf588vmm9jy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM MODEL ARCHITECTURE\n","\n","model = keras.models.Sequential([\n","  keras.Input(shape=(20, 1)),\n","  # Masking layer, to ignore zeros.\n","  # keras.layers.Masking(),\n","  # 4 LSTM Layers with 16 units.\n","  keras.layers.LSTM(units=16, return_sequences=True),\n","  keras.layers.LSTM(units=16, return_sequences=True),\n","  keras.layers.LSTM(units=16, return_sequences=True),\n","  keras.layers.LSTM(units=16, dropout=0.5, return_sequences=True),\n","  # Fully Connected Layer\n","  keras.layers.Dense(units=1)\n","])\n","model.summary()"],"metadata":{"id":"3xYTfnX7mRcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss and Optimizer Functions\n","mse = tf.keras.losses.MeanSquaredError()\n","rmse = tf.keras.metrics.RootMeanSquaredError()\n","model.compile(loss=mse, optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=[rmse])"],"metadata":{"id":"efwTmK16oM0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Callbacks.\n","\n","callbacks = [EarlyStopping(monitor='val_loss', patience=100, verbose=1), \n","            ModelCheckpoint(filepath='/content/sample_data/lstm-model.h5', verbose=1, monitor='val_loss', save_best_only=True, save_weights_only=False)]\n","\n","# Train the model\n","history = model.fit(x=X_train, y=Y_train, validation_data =(X_val, Y_val), epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=callbacks)"],"metadata":{"id":"ADW-xwhOmRel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model evaluation\n","test_predicted = model.predict(X_test)"],"metadata":{"id":"pb7syksNmRhG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply inverse transformation\n","test_predicted= scaler.inverse_transform(test_predicted)\n"],"metadata":{"id":"--JQahZGmRml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate RMSE\n","math.sqrt(mean_squared_error(Y_test,test_predicted))\n"],"metadata":{"id":"qZQEG9Z3pLkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting everything together\n","\n","look_back=20\n","# shift test predictions for plotting\n","test_predicted_plot = np.empty_like(df1)\n","test_predicted_plot[:, :] = np.nan\n","test_predicted_plot[validation_size+(look_back*2)+1:len(df1)-1, :] = test_predicted\n","# plot baseline and predictions\n","plt.plot(scaler.inverse_transform(df1))\n","plt.plot(test_predicted_plot)\n","plt.show()"],"metadata":{"id":"475ZyW85pnN0"},"execution_count":null,"outputs":[]}]}